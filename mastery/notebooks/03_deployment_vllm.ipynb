{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 03 · Despliegue con vLLM\n\n**Objetivo:** servir un modelo con vLLM y comparar throughput vs. `transformers`.\n\n**Índice:** 1) Setup 2) Servir con vLLM (CLI/API) 3) Cliente de prueba 4) Benchmark simple 5) Conclusiones"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Setup"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": "# !pip install vllm uvicorn fastapi requests transformers\nimport os, time, requests\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Iniciar servidor vLLM (línea de comandos)\n> Este paso suele ejecutarse en terminal separada."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": "# Ejemplo:\n# !python -m vllm.entrypoints.openai.api_server \\\n#   --model <modelo> \\\n#   --port 8000 \\\n#   --tensor-parallel-size 1 \\\n#   --max-num-batched-tokens 4096\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Cliente (OpenAI-compatible)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": "import json\n\nOPENAI_BASE = \"http://localhost:8000/v1\"\nAPI_KEY = \"EMPTY\"\n\n# def chat(prompt: str):\n#     url = f\"{OPENAI_BASE}/chat/completions\"\n#     payload = {\"model\": \"<modelo>\", \"messages\": [{\"role\":\"user\",\"content\":prompt}]}\n#     headers = {\"Authorization\": f\"Bearer {API_KEY}\", \"Content-Type\": \"application/json\"}\n#     r = requests.post(url, json=payload, headers=headers, timeout=60)\n#     return r.json()\n\n# chat(\"Hola vLLM, ¿qué tal?\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Benchmark simple"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": "from time import perf_counter\n\nPROMPTS = [\"Resume en 2 frases la teoría de atención.\"]*5\n\n# def bench(func):\n#     t0 = perf_counter()\n#     for p in PROMPTS:\n#         _ = func(p)\n#     t1 = perf_counter()\n#     print(f\"Tiempo total: {t1 - t0:.2f}s · avg: {(t1 - t0)/len(PROMPTS):.2f}s\")\n\n# bench(chat)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Conclusiones\n- Anota TPS/latencia y configuración (batch size, paged attention)."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}