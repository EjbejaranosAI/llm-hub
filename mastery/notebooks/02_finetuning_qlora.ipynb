{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# 02 · Fine-tuning Eficiente con QLoRA\n\n**Objetivo:** entrenar con QLoRA sobre un dataset pequeño (ej. Alpaca subset) y evaluar pérdidas.\n\n**Índice:** 1) Setup 2) Dataset 3) Configuración QLoRA 4) Entrenamiento 5) Evaluación 6) Exportación"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Setup"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": "# !pip install torch transformers datasets peft accelerate bitsandbytes trl\nimport os\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Dataset"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": "# from datasets import load_dataset\n# ds = load_dataset(\"tatsu-lab/alpaca\")\n# ds_small = ds['train'].select(range(1000))\n# ds_small\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Configuración QLoRA"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": "# from transformers import AutoModelForCausalLM, AutoTokenizer\n# from peft import LoraConfig, get_peft_model\n# MODEL_NAME = \"<modelo-base>\"\n# tok = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n# base = AutoModelForCausalLM.from_pretrained(MODEL_NAME, load_in_4bit=True, device_map=\"auto\")\n# lora_cfg = LoraConfig(r=64, lora_alpha=16, target_modules=[\"q_proj\",\"v_proj\"], lora_dropout=0.05, bias=\"none\")\n# model = get_peft_model(base, lora_cfg)\n# model.print_trainable_parameters()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Entrenamiento (TRL / Trainer)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": "# from transformers import TrainingArguments, Trainer\n# def format_sample(example):\n#     return {\"text\": f\"Instrucción: {example['instruction']}\\nEntrada: {example.get('input','')}\\nRespuesta:\"}\n# # ds_fmt = ds_small.map(format_sample)\n# training_args = TrainingArguments(\n#     output_dir=\"./outputs/qlora\",\n#     per_device_train_batch_size=1,\n#     gradient_accumulation_steps=8,\n#     learning_rate=2e-4,\n#     num_train_epochs=1,\n#     logging_steps=10,\n#     fp16=True\n# )\n# trainer = Trainer(model=model, args=training_args, train_dataset=None)  # TODO: ds_fmt\n# # trainer.train()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Evaluación rápida"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": "# TODO: eval simple con prompts de sanity-check\n# prompts = [\"¿Qué es QLoRA?\", \"Explica PEFT en una frase.\"]\n# for p in prompts:\n#     pass\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Exportación y carga de adaptadores"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": "# model.save_pretrained(\"outputs/qlora-adapter/\")\n# # Para cargar luego:\n# # from peft import PeftModel\n# # base = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n# # model = PeftModel.from_pretrained(base, \"outputs/qlora-adapter/\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Notas\n- Ajusta `r`, `lora_alpha` y `target_modules` según el modelo.\n- Controla VRAM con batch pequeño y acumulación de gradientes."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}